method: 
  module: methods
  method: CLStrategy
  args:
    model:
      module: timm.models
      method: resnet50d
      args:
        pretrained: True
      head_layer: fc
      embedding_dims: 2048

    optimizer:
      module: torch.optim
      method: SGD
      args:
        lr: 0.0111
    
    criterion:
      module: ocl.loss
      method: ClassBalancedLoss
      args:
        num_classes: 7
        beta: 0.81
        loss: 
          module: ocl.loss
          method: FocalLoss
          args:
            gamma: 0.0
            alpha: 0.5
            reduction: mean
    
    lr_scheduler:
      module: torch.optim.lr_scheduler
      method: MultiStepLR
      args: 
        milestones : [1000, 2000]
        gamma: 0.1

    plugins:
      module: class_strategy
      method: ClassStrategyPlugin
      args: 
        mem_size: 1000
        sweep_memory_every_n_iter: 1000
        memory_sweep_default_size: 500
        num_samples_per_batch: 5
        target_layer: global_pool
        ep_memory_batch_size: 6
        memory_transforms: [
        "transforms.RandomHorizontalFlip()",
        "transforms.RandomRotation(10)",
        "transforms.ColorJitter(brightness=0.5, hue=0.5)",
        "transforms.ToTensor()",
        "transforms.Normalize((0.3252, 0.3283, 0.3407), (0.0265, 0.0241, 0.0252))"]

        # online sampler
        online_sampler: 
          module: ocl.sampler
          method: UncertaintySampler
          args:
            num_workers: 10
            scoring_method: entropy
            negative_mining: True
          
        # periodic sampler
        periodic_sampler:
          module: ocl.sampler
          method: RMSampler
          args:
            augmentation: vr_randaug
            batch_size: 32
            num_workers: 10
        
        # softlabels learning
        softlabels_trainer:
          module: ocl.modules
          method: SoftLabelsLearningTrainer
          args:
            temperature: 0.5
            patience: 1700
            loss_weights:
              milestones: [1700, 2000, 2100]
              cross_entropy: [1.0, 1.0, 1.0]
              kl_divergence: [0.5, 0.8, 1.0]

        # metric learning
        metric_learning:
          module: ocl.modules
          method: MetricLearner
          args:
            losses: 
              - module: pytorch_metric_learning.losses
                method: ContrastiveLoss
                args: 
                  pos_margin: 0.0
                  neg_margin: 1.0
                weight: 0.3
              - module: pytorch_metric_learning.losses
                method: SupConLoss
                args:
                  temperature: 0.1
                weight: 0.1
        
        # episodic memory dataloader sampler
        memory_dataloader_sampler:
          module: torchsampler
          method: ImbalancedDatasetSampler
        
        # augmentation
        augmentation:
          module: ocl.modules
          method: MixUp
          args: 
            alpha: 1.0

        # fine tune head
        finetune_head:
          module: ocl.modules
          method: FinetuneHeadTrainer
          args:
            criterion:
              module: torch.nn 
              method: CrossEntropyLoss
              args:
                reduction: mean

            optimizer:
              module: torch.optim
              method: SGD
              args:
                lr: 0.0001

            lr_scheduler:
              module: torch.optim.lr_scheduler
              method: MultiStepLR
              args: 
                milestones : [1000, 2000]
                gamma: 0.1
                
            layer_head: fc

            dataloader:
              module: torch.utils.data
              method: DataLoader
              args:
                batch_size: 6
                shuffle: False
                num_workers: 8
                drop_last: True

            dataloader_sampler:
              module: ocl.sampler
              method: ClassAwareSampler
              args: 
                sample_per_class: 1

            softlabels_trainer:
              module: ocl.modules
              method: SoftLabelsLearningTrainer
              args:
                temperature: 0.5
                patience: 1700
                loss_weights:
                  milestones: [1700, 2000, 2100]
                  cross_entropy: [1.0, 1.0, 1.0]
                  kl_divergence: [0.5, 0.8, 1.0]

    logger:
      module: wandb
      method: init
      args: 
        project: sslad-iccv-2021
        name: FinetuneHead-sota
        entity: mrifkikurniawan
        save_code: True
        tags: [replay, imbalanced sampler, multistep lr scheduler, softlabels, contrastive learning, SupConLoss, resnet50d, ClassBalancedFocalLoss, FinetuneHead, sota]
        notes: FinetuneHead


hparams_optimizer:
  direction: maximize
  n_trials: 20
  hparams:
    - name: plugins.args.metric_learning.args.losses.weight
      index: 1
      method: suggest_discrete_uniform
      trial_args: 
        name: TripletMarginLoss_weight
        low: 0.1
        high: 1.0
        q: 0.1