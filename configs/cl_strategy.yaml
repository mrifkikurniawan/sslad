method: 
  module: methods
  method: CLStrategy
  args:
    model:
      module: timm.models
      method: resnet50d
      args:
        pretrained: True
      head_layer: fc
      embedding_dims: 2048

    optimizer:
      module: torch.optim
      method: SGD
      args:
        lr: 0.0111
    
    criterion:
      module: loss
      method: CostSenstiveLoss
      args:
        num_classes: 7
        loss: 
          module: torch.nn.functional 
          method: cross_entropy
    
    lr_scheduler:
      module: torch.optim.lr_scheduler
      method: MultiStepLR
      args: 
        milestones : [1000, 2000]
        gamma: 0.1

    plugins:
      module: class_strategy
      method: ClassStrategyPlugin
      args: 
        mem_size: 1000
        sweep_memory_every_n_iter: 1000
        memory_sweep_default_size: 500
        num_samples_per_batch: 5
        cut_mix: False
        temperature: 0.5
        target_layer: global_pool
        softlabels_patience: 1700
        ep_memory_batch_size: 6
        memory_transforms: [
        "transforms.RandomHorizontalFlip()",
        "transforms.RandomRotation(10)",
        "transforms.ColorJitter(brightness=0.5, hue=0.5)",
        "transforms.ToTensor()",
        "transforms.Normalize((0.3252, 0.3283, 0.3407), (0.0265, 0.0241, 0.0252))"]

        # online sampler
        online_sampler: 
          module: modules
          method: UncertaintySampler
          args:
            num_workers: 10
            scoring_method: entropy
            negative_mining: True
          
        # periodic sampler
        periodic_sampler:
          module: modules
          method: RMSampler
          args:
            augmentation: vr_randaug
            batch_size: 32
            num_workers: 10
        
        # softlabels learning
        loss_weights: 
          milestones: [1700, 2000, 2100]
          cross_entropy: [1.0, 1.0, 1.0]
          kl_divergence: [0.5, 0.8, 1.0]

        # metric learning
        metric_learning:
          module: modules
          method: MetricLearner
          args:
            losses: 
              - module: pytorch_metric_learning.losses
                method: ContrastiveLoss
                args: 
                  pos_margin: 0.0
                  neg_margin: 1.0
                weight: 0.3
              - module: pytorch_metric_learning.losses
                method: SupConLoss
                args:
                  temperature: 0.1
                weight: 0.1


    logger:
      module: wandb
      method: init
      args: 
        project: sslad-iccv-2021
        name: CostSenstiveCrossEntropy
        entity: mrifkikurniawan
        save_code: True
        tags: [replay, imbalanced sampler, multistep lr scheduler, softlabels, contrastive learning, SupConLoss, resnet50d, CostSenstiveCrossEntropy]
        notes: CostSenstiveCrossEntropy


hparams_optimizer:
  direction: maximize
  n_trials: 20
  hparams:
    - name: plugins.args.metric_learning.args.losses.weight
      index: 1
      method: suggest_discrete_uniform
      trial_args: 
        name: TripletMarginLoss_weight
        low: 0.1
        high: 1.0
        q: 0.1